---
title: 'About RAG'
description: "A quick primer on Retrieval Augmented Generation (RAG)"
---

<img
    src="/images/RAG-Illustration-1.svg"
    alt="RAG Illustration"
/>
### Introduction
Retrieval Augmented Generation (RAG) is a technique designed to enhance LLMs by integrating external knowledge bases into the response generation process. While LLMs are trained on large datasets, they often lack domain-specific or real-time knowledge about certain topics. RAG addresses these gaps by retrieving relevant information from external sources and incorporating it into the model's responses.

At Wetrocloud, we leverage RAG behind the scenes to enrich responses with contextually relevant data from external knowledge bases. This approach enables the system to adapt to diverse use cases, making it a good alternative for scenarios requiring specialized information.

### RAG: A Mental ðŸ§  Model to Understand
Imagine youâ€™re writing an essay, but instead of relying on just your memory, you have a library next to you. When you come across a question or topic youâ€™re unsure about, you pause, walk over to the library, find the most relevant book, and use the information to continue writing your essay.  

In this analogy:  
- **You** are the language model (LLM), trained on a lot of knowledge but limited to what you already know.  
- **The library** is the external knowledge base, full of updated and domain-specific information.  
- **RAG** is the process of going to the library, picking the right book, and using its content to create a more informed essay.  

### Usecases of RAG
RAG has become incredibly popular these days, Some common use cases of RAG include:
- Customer service chatbots
- Virtual assistants
- Enterprise search engines
- Document summarization
- Conversational chatbots

### Further Reading
- [The official RAG Paper](https://arxiv.org/abs/2005.11401)
- [How RAG works behind the scenes](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)

